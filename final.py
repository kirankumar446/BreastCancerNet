# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MHgH8i01kDG4hJVQUYimbrD-q9yet4TY
"""

#Implementing the Dual DNN (DNN-I + DNN-II) -> Concatenation -> Attention -> SVM
#with explicit CSV data loading + preprocessing, as in hybridmodelfinal.py.

import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc

import tensorflow as tf
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, Concatenate, Multiply, Lambda
from tensorflow.keras.callbacks import EarlyStopping


# ----------------- MODEL BUILDING -----------------

def build_dnn_i(x):
    """DNN-I: simple stacked Dense + Dropout blocks (top branch)."""
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.25)(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.2)(x)
    out = Dense(16, activation='linear')(x)
    return out


def build_dnn_ii(x):
    """DNN-II: Dense -> BN -> ReLU -> Dropout blocks (bottom branch)."""
    x = Dense(128)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)

    x = Dense(64)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.25)(x)

    x = Dense(32)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.2)(x)

    out = Dense(16, activation='linear')(x)
    return out


def attention_block(features):
    """Lightweight attention mechanism across feature dimensions."""
    dim = int(features.shape[-1])
    scores = Dense(dim, activation='tanh')(features)
    scores = Dense(dim)(scores)
    scores = Activation('softmax')(scores)
    weighted = Multiply()([features, scores])
    context = Lambda(lambda z: tf.reduce_sum(z, axis=1, keepdims=False))(weighted)
    return context


def build_model(input_dim):
    inp = Input(shape=(input_dim,))
    dnn1_out = build_dnn_i(inp)
    dnn2_out = build_dnn_ii(inp)

    concat = Concatenate()([dnn1_out, dnn2_out])
    attn_context = attention_block(concat)
    final_feature_vec = Dense(32, activation='relu')(attn_context)
    output = Dense(1, activation='sigmoid')(final_feature_vec)

    model = Model(inputs=inp, outputs=output)
    feature_extractor = Model(inputs=inp, outputs=final_feature_vec)
    return model, feature_extractor


# ----------------- DATA LOADING + PREPROCESSING -----------------

def load_and_preprocess(csv_path):
    """Load dataset from CSV, encode labels, scale features."""
    df = pd.read_csv(csv_path)

    # Adjust if label column differs
    if 'target' not in df.columns and 'label' in df.columns:
        df.rename(columns={'label': 'target'}, inplace=True)
    if 'target' not in df.columns:
        raise ValueError("CSV must contain a 'target' column (0/1 labels).")

    X = df.drop(columns=['target']).values
    y = df['target'].values

    # Encode if not numeric
    if y.dtype == object:
        y = LabelEncoder().fit_transform(y)

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # MinMax scaling for NN
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler


# ----------------- MAIN -----------------

def main(args):
    # Load dataset
    X_train, X_test, y_train, y_test, input_scaler = load_and_preprocess(args.csv)
    print(f"Data loaded: {X_train.shape[0]} train, {X_test.shape[0]} test, {X_train.shape[1]} features")

    # Build and train NN
    model, feature_extractor = build_model(X_train.shape[1])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    es = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True, verbose=1)
    model.fit(X_train, y_train, validation_split=0.15,
              epochs=200, batch_size=32, callbacks=[es], verbose=2)

    # Extract attention-based features
    train_feat = feature_extractor.predict(X_train)
    test_feat = feature_extractor.predict(X_test)

    # Standardize before SVM
    feat_scaler = StandardScaler()
    train_feat_std = feat_scaler.fit_transform(train_feat)
    test_feat_std = feat_scaler.transform(test_feat)

    # Train + evaluate SVM
    svm = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)
    svm.fit(train_feat_std, y_train)
    y_pred = svm.predict(test_feat_std)

    print("\n--- Results ---")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("Classification Report:\n", classification_report(y_test, y_pred, digits=4))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

    # ROC curve
    try:
        proba = svm.predict_proba(test_feat_std)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, proba)
        roc_auc = auc(fpr, tpr)
        print(f"ROC AUC: {roc_auc:.4f}")

        plt.figure()
        plt.plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.3f}')
        plt.plot([0, 1], [0, 1], linestyle='--')
        plt.xlabel('FPR')
        plt.ylabel('TPR')
        plt.title('ROC - SVM on Attention Features')
        plt.legend()
        plt.show()
    except Exception as e:
        print("ROC computation failed:", e)

    # Save models
    if args.save:
        model.save('dual_dnn_attn_full_nn.h5')
        joblib.dump(svm, 'svm_on_attn_features.joblib')
        joblib.dump(feat_scaler, 'attn_feature_scaler.joblib')
        joblib.dump(input_scaler, 'input_scaler.joblib')
        print("Models saved!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", type=str, required=True, help="Path to dataset CSV")
    parser.add_argument("--save", action="store_true", help="Save models and scalers")
    args = parser.parse_args()
    main(args)