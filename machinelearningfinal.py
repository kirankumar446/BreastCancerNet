# -*- coding: utf-8 -*-
"""MachineLearningFINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H6fAK36S-kaQrOX_SSnquLzX6BxYA2_c

#Drive Mount: Establishing Data Accessibility
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""#Required Libraries:"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Pre-Modelling
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.svm import SVC

# Evaluation and comparision of all the models
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import precision_score, recall_score, confusion_matrix, precision_recall_fscore_support
from sklearn.metrics import roc_auc_score,auc,f1_score
from sklearn.metrics import precision_recall_curve,roc_curve
from keras import models
from keras.layers import Dense, Conv1D, Attention, Flatten, Dropout, LSTM
from keras.models import Sequential,Model
from keras import utils
import tensorflow as tf
import pydot
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
#tensorflow
!pip install tensorflow
!pip install keras
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.regularizers import l2  # Import regularizers from TensorFlow directly
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

"""#Data reading:"""

df = pd.read_csv('/content/gdrive/MyDrive/Dataset/Breastcancer.csv')

B,M = df['diagnosis'].value_counts()
print(f"Benign = {B}, Malignant = {M}")

plt.rcParams['figure.figsize'] = (8,5)
sns.countplot(x="diagnosis", data=df)
plt.subplot().set_xticklabels(["Malignant", "Benign"])

"""# Dropping the id coloumn"""

df.drop(columns='id', inplace=True)

df.describe()

"""# Data preprocessing:"""

y=df.diagnosis
X = df.drop(['diagnosis'], axis=1)

df.columns.isnull()

"""#Train Test Split:"""

x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

sc = StandardScaler()
X_train = sc.fit_transform(x_train)
X_test= sc.transform(x_test)

"""#Model Creation:"""

# Encode the labels in y_train and y_test as integers
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.transform(y_test)

# Fit the LogisticRegression model
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Predict using the fitted model
y_pred = logreg.predict(X_test)

# Logistic Regression
logreg= LogisticRegression()
logreg.fit(X_train, y_train)
y_pred= logreg.predict(X_test)

# Gradient Boosting Classifier
GB = GradientBoostingClassifier()
GB.fit(X_train, y_train)
y_pred_GB = GB.predict(X_test)

# Random Forest Classifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# Decision Tree Classifier
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

# KNeighbors Classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

# XGB Classifier
XGB = XGBClassifier()
XGB.fit(X_train, y_train)
y_pred_XGB = XGB.predict(X_test)

# Support Vector classifier
#svc = SVC(probability=True)
#svc.fit(X_train,y_train)
#y_pred_svc = svc.predict(X_test)

#Support Vector Machine
svm_classifier = SVC()
svm_classifier.fit(X_train, y_train)
y_pred_svm = svm_classifier.predict(X_test)
conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)

# Your list of models
Z = [SVC(), DecisionTreeClassifier(), LogisticRegression(), KNeighborsClassifier(),
     XGBClassifier(), RandomForestClassifier(), GradientBoostingClassifier()]

X = ["SVC", "DecisionTreeClassifier", "LogisticRegression", "KNeighborsClassifier",
     "RandomForestClassifier", "GradientBoostingClassifier", "XGB"]

# Initialize an empty list to store accuracy scores
models = []

# Loop through models, fit, predict, and store accuracy scores
for i in range(len(Z)):
    model = Z[i]
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    models.append(accuracy_score(pred, y_test))

# Create a DataFrame
d = {"Accuracy": models, "Algorithm": X}
data_frame = pd.DataFrame(d)

# Sort the DataFrame by 'Accuracy' column in descending order
sorted_data_frame = data_frame.sort_values(by='Accuracy', ascending=False)

# Display the sorted DataFrame
print(sorted_data_frame)

"""#Confusion Matrix:"""

# List of classifiers
classifiers = [
    ('Logistic Regression', LogisticRegression()),
    ('Gradient Boosting', GradientBoostingClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('Decision Tree', DecisionTreeClassifier()),
    ('KNN', KNeighborsClassifier(n_neighbors=5)),
    ('XGBoost', XGBClassifier()),
    ('SVC', SVC(probability=True)),
    ('SVM', SVC())
]

# Iterate through classifiers
for classifier_name, classifier in classifiers:
    # Train the classifier
    classifier.fit(X_train, y_train)

    # Make predictions
    y_pred = classifier.predict(X_test)

    # Calculate confusion matrix
    conf_matrix = confusion_matrix(y_test, y_pred)

    # Create a Pandas DataFrame from the confusion matrix
    conf_matrix_df = pd.DataFrame(conf_matrix,
                                  index=['Benign', 'Malignant'],
                                  columns=['Benign', 'Malignant'])

    # Print the numerical values
    print(f"\nConfusion Matrix for {classifier_name}:")
    print(conf_matrix_df)

    # Plot Confusion Matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['Benign', 'Malignant'],
                yticklabels=['Benign', 'Malignant'])
    plt.title(f'Confusion Matrix for {classifier_name}')
    plt.xlabel('Predicted Values')
    plt.ylabel('Actual Values')
    plt.show()

"""#ROC curves:"""

import matplotlib.pyplot as plt
# Create a list of classifiers
classifiers = [
    ('Logistic Regression', LogisticRegression()),
    ('Gradient Boosting', GradientBoostingClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('Decision Tree', DecisionTreeClassifier()),
    ('KNN', KNeighborsClassifier(n_neighbors=5)),
    ('XGBoost', XGBClassifier()),
    ('SVM', SVC())
]

# Iterate through classifiers and plot ROC curves
for classifier_name, classifier in classifiers:
    # Train the classifier
    classifier.fit(X_train, y_train)

    # Make predictions
    y_pred = classifier.predict(X_test)

    # Calculate ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', label=f'{classifier_name}(area = {roc_auc:.2f})')
    #plt.plot(fpr, tpr, color='darkorange', label=' DNN-II (AUC = %0.2f)' % roc_auc)
    # Plot the diagonal 50% line
    plt.plot([0, 1], [0, 1], 'k--')

    # Set axis labels and title
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for {classifier_name}')

    # Add legend
    plt.legend()

    # Display the plot
    plt.show()

import matplotlib.pyplot as plt
# Create a list of classifiers
classifiers = [
    ('Logistic Regression', LogisticRegression()),
    ('Gradient Boosting', GradientBoostingClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('Decision Tree', DecisionTreeClassifier()),
    ('KNN', KNeighborsClassifier(n_neighbors=5)),
    ('XGBoost', XGBClassifier()),
    ('SVM', SVC())
]

# Iterate through classifiers and plot ROC curves
for classifier_name, classifier in classifiers:
    # Train the classifier
    classifier.fit(X_train, y_train)

    # Make predictions
    y_pred = classifier.predict(X_test)

    # Calculate ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_pred)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.plot(fpr, tpr, label=f'{classifier_name} (area = {roc_auc:.2f})')

# Plot the diagonal 50% line
plt.plot([0, 1], [0, 1], 'k--')

# Set axis labels and title
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Different Classifiers')

# Add legend
plt.legend()

# Display the plot
plt.show()

"""#Graphs"""

import matplotlib.pyplot as plt
# Create a bar plot of classifier accuracies
plt.figure(figsize=(10, 6))
plt.bar(X, models, color=['skyblue'])
plt.xticks(rotation=45, ha='right')
plt.xlabel('Machine Learning Classifiers')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison')

# Add accuracy values on top of the bars
for i, v in enumerate(models):
    plt.text(i, v + 0.01, str(round(v * 100, 2)) + '%', ha='center')

# Replace SVC classifier as SVM
X[0] = 'SVM'

# Sort the x-axis labels in descending order of accuracy
X = sorted(X, key=lambda x: models[X.index(x)], reverse=True)

# Reorder the bars according to the sorted x-axis labels
plt.xticks(range(len(X)), X)

# Display the plot
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn  as  sns

cm = np.array([[61,2],[1,107]])


plt.figure(figsize=(8,6))
plt.rcParams.update({'font.size': 16})
#sns.set(font_scale=2.0)
sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', cbar=True,
                xticklabels=['Benign', 'Malignant'],
                yticklabels=['Benign', 'Malignant'])
#plt.title("Confusion Matrix")
plt.ylabel('Actual Values')
plt.xlabel('Predicted Values')
plt.savefig('malariacm.eps', format='eps')

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Confusion matrix
cm = np.array([[62, 0], [1, 108]])

# Plotting the heatmap
plt.figure(figsize=(8, 6))
plt.rcParams.update({'font.size': 18})

# Create heatmap with annotation font size control
ax = sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', cbar=True,
                 xticklabels=['Benign', 'Malignant'],
                 yticklabels=['Benign', 'Malignant'],
                 annot_kws={"size": 18})  # Control annotation font size

# Setting font size for x and y tick labels
ax.set_xticklabels(ax.get_xticklabels(), fontsize=16)
ax.set_yticklabels(ax.get_yticklabels(), fontsize=16)

# Labeling
plt.ylabel('Actual Values', fontsize=18)
plt.xlabel('Predicted Values', fontsize=18)
plt.savefig('CFHyBCNEt.eps', format='eps')
plt.show()

from google.colab import files

# Download the EPS file
files.download('CFHyBCNEt.eps')